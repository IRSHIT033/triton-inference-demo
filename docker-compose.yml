x-gpu-env: &gpu_env
  NVIDIA_VISIBLE_DEVICES: "all"
  NVIDIA_DRIVER_CAPABILITIES: "compute,utility"

x-gpu-deploy: &gpu_deploy
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            capabilities: ["gpu"]

services:
  # 1) Build the TensorRT engine (.plan) from your ONNX
  trt_builder:
    image: nvcr.io/nvidia/tensorrt:24.05-py3
    profiles: ["builder"]
    working_dir: /workspace
    environment: *gpu_env
    <<: *gpu_deploy
    volumes:
      - ./artifacts:/workspace/artifacts # expects artifacts/dinov2.onnx
      - ./models:/workspace/models # will write models/dinov2/1/model.plan
    command: >
      bash -c "
      mkdir -p models/dinov2/1 &&
      trtexec --onnx=/workspace/artifacts/dinov2.onnx --saveEngine=/workspace/models/dinov2/1/model.plan --minShapes=input:1x3x224x224 --optShapes=input:8x3x224x224 --maxShapes=input:32x3x224x224 --fp16 --workspace=4096 --verbose
      "

  # 2) Run Triton with the baked model repo (preproc + dinov2 + ensemble)
  triton:
    image: nvcr.io/nvidia/tritonserver:24.05-py3
    profiles: ["triton"]
    working_dir: /workspace
    environment: *gpu_env
    <<: *gpu_deploy
    ports:
      - "8000:8000" # HTTP
      - "8001:8001" # gRPC
      - "8002:8002" # Metrics
    volumes:
      - ./models:/workspace/models
    command: >
      tritonserver
        --model-repository=/workspace/models
        --exit-on-error=false
        --disable-auto-complete-config

  # 3) Optional: minimal HTTP client you can "run" via Compose
  client:
    image: python:3.11-slim
    profiles: ["client"]
    working_dir: /app
    volumes:
      - ./clients:/app
    environment:
      TRITON_URL: "http://triton:8000"
    command: >
      bash -lc "
        pip install --no-cache-dir numpy tritonclient[http] opencv-python-headless &&
        python client_ensemble.py
      "
    depends_on:
      - triton
