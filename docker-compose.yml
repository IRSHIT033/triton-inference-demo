x-gpu-env: &gpu_env
  NVIDIA_VISIBLE_DEVICES: "all"
  NVIDIA_DRIVER_CAPABILITIES: "compute,utility"

x-gpu-deploy: &gpu_deploy
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            capabilities: ["gpu"]

services:
  # 1) Build the TensorRT engine (.plan) from your ONNX
  trt_builder:
    image: nvcr.io/nvidia/tensorrt:24.05-py3
    profiles: ["builder"]
    working_dir: /workspace
    environment: *gpu_env
    <<: *gpu_deploy
    volumes:
      - .:/workspace # mount entire project directory
      - ./models:/workspace/models # will write models/dinov2/1/model.plan
    command: >
      bash -c "
      pip install -r requirements.txt &&
      python export_model_onnx.py &&
      mkdir -p models/dinov2/1 &&
      trtexec --onnx=dinov2_base_cls.onnx --saveEngine=/workspace/models/dinov2/1/model.plan --minShapes=input:1x3x224x224 --optShapes=input:8x3x224x224 --maxShapes=input:32x3x224x224 --fp16 --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --verbose
      "

  # 2) Run Triton with the baked model repo (preproc + dinov2 + ensemble)
  triton:
    image: nvcr.io/nvidia/tritonserver:24.05-py3
    profiles: ["triton"]
    working_dir: /workspace
    environment: *gpu_env
    <<: *gpu_deploy
    ports:
      - "8000:8000" # HTTP
      - "8001:8001" # gRPC
      - "8002:8002" # Metrics
    volumes:
      - ./models:/workspace/models
    command: >
      bash -c "
      pip install Pillow &&
      tritonserver --model-repository=/workspace/models --exit-on-error=false --disable-auto-complete-config
      "

  # 3) Optional: minimal HTTP client you can "run" via Compose
  client:
    image: python:3.11-slim
    profiles: ["client"]
    working_dir: /app
    volumes:
      - .:/app
    environment:
      TRITON_URL: "http://triton:8000"
    command: >
      bash -lc "
        pip install --no-cache-dir -r client_requirements.txt &&
        python client.py
      "
    depends_on:
      - triton
