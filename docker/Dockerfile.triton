# Triton Inference Server with TensorRT support for DINO V2
FROM nvcr.io/nvidia/tritonserver:23.10-py3

# Install additional dependencies
RUN pip install --no-cache-dir \
    torch==2.1.0 \
    torchvision==0.16.0 \
    transformers==4.36.0 \
    timm==0.9.12 \
    opencv-python==4.8.1.78 \
    pillow==10.1.0

# Create model repository directory
RUN mkdir -p /models

# Copy model repository
COPY model_repository/ /models/

# Set model repository path
ENV TRITON_MODEL_REPOSITORY=/models

# Expose Triton ports
EXPOSE 8000 8001 8002

# Start Triton server
CMD ["tritonserver", "--model-repository=/models", "--strict-model-config=false", "--log-verbose=1"] 